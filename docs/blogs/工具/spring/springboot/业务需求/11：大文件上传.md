---
title: 11：大文件上传
date: 2023-11-20
sidebar: auto
categories:
  - spring
tags:
  - Java
  - springboot

author: 胡昊泽
---

::: warning 前言
这种大文件上传一般采用 **分片上传**、 **断点续传**、 **秒传** 等。那他们都解决什么问题呢？

**分片上传**、**断点续传**：主要解决网络突然断开，那么我下次上传该文件又要重新开始上传？肯定不用，**分片上传** 就是可以存储此次该文件已经上传到了那个地方，**断点续传** 就是下次我可以接着从这个地方继续上传。

**秒传**：是前端会对该文件进行md5加密， **基本上相同文件得md5都相等** ，我后端会根据分片数量是否相等，已经该文件是否存在来认为是否需要上传，相等则代表文件已存在，则不需要上传，直接显示成功，这就是妙传！
:::

## 1：思路实现

### 1）前端思路

- 1：前端实现对文件进行 **md5加密**（加密的是整个文件，而不是分片文件）  
- 2：采用 **两个插件（vue-simple-uploader、spark-md5）** 进行具体实现  
- 3：主要调用两个接口，一个是 **分片上传接口**，一个是 **分片合并接口** 
- 4：主要的八个参数：**文件** ，**文件名** ，**文件md加密值** ，**文件大小** ，**分片总数** ，**分片大小** ，**具体上传的分片大小** ，**当前块的序号** 

**`chunkNumber: `**  当前块的次序，第一个块是 1，注意不是从 0 开始的。  
**`totalChunks: `**  文件被分成块的总数。  
**`chunkSize: `**  分块大小，根据 totalSize 和这个值可以计算出总共的块数。注意最后一块的大小可能会比这个要大。  
**`currentChunkSize: `**  当前块的大小，实际大小。  
**`totalSize: `**  文件总大小。  
**`identifier: `**  这个就是MD5值，每个文件的唯一标示。  
**`filename: `**  文件名  

### 2）后端上传文件校验思路
- 1：检查 **文件是否存在**，如果存在则跳过该文件的上传，如果不存在，返回需要上传的分片集合
- 2：检查 **分片是否存在** 
  -  1：检查目录下的 **文件是否存在**。
  -  2：检查 **redis** 存储的分片是否存在。
  -  3：判断 **分片数量** 和 **总分片数量** 是否一致。
     - 1：如果 **文件存在并且分片上传完毕** ，标识已经完成附件的上传，可以进行秒传操作。
     - 2：如果 **文件不存在或者分片未上传完毕** ，则返回false并返回已经上传的分片信息。  
### 3）后端分片上传文件思路
- 判断 **目录是否存在** ，如果不存在则创建目录。
- 进行 **切片的拷贝**，将 **切片拷贝到指定的目录** 。
- 将该分片写入 **redis**
### 4）后端合成分片文件思路
## 2：前端实现

安装依赖
```npm
npm install --save vue-simple-uploader
npm install --save spark-md5
``` 

mainjs导入uploader
```js
import uploader from 'vue-simple-uploader'
Vue.use(uploader)
```

```vue
<template>
  <div>
    <uploader
      :autoStart="false"
      :options="options"
      :file-status-text="statusText"
      class="uploader-example"
      @file-complete="fileComplete"
      @complete="complete"
      @file-success="fileSuccess"
      @files-added="filesAdded"
    >
      <uploader-unsupport></uploader-unsupport>
      <uploader-drop>
        <p>将文件拖放到此处以上传</p>
        <uploader-btn>选择文件</uploader-btn>
        <uploader-btn :attrs="attrs">选择图片</uploader-btn>
        <uploader-btn :directory="true">选择文件夹</uploader-btn>
      </uploader-drop>
      <!-- <uploader-list></uploader-list> -->
      <uploader-files> </uploader-files>
    </uploader>
    <br />
    <el-button @click="allStart()" :disabled="disabled">全部开始</el-button>
    <el-button @click="allStop()" style="margin-left: 4px">全部暂停</el-button>
    <el-button @click="allRemove()" style="margin-left: 4px">全部移除</el-button>
  </div>
</template>

<script>
import axios from "axios";
import SparkMD5 from "spark-md5";
// import {upload} from "@/api/user";
// import storage from "store";
// import { ACCESS_TOKEN } from '@/store/mutation-types'
export default {
  name: "UploadFile",
  data() {
    return {
      skip: false,
      options: {
        target: "//localhost:8160/upload/chunk",
        // 开启服务端分片校验功能
        testChunks: true,
        parseTimeRemaining: function (timeRemaining, parsedTimeRemaining) {
          return parsedTimeRemaining
            .replace(/\syears?/, "年")
            .replace(/\days?/, "天")
            .replace(/\shours?/, "小时")
            .replace(/\sminutes?/, "分钟")
            .replace(/\sseconds?/, "秒");
        },
        // 服务器分片校验函数
        checkChunkUploadedByResponse: (chunk, message) => {
          const result = JSON.parse(message);
          if (result.data.skipUpload) {
            this.skip = true;
            return true;
          }
          return (result.data.uploaded || []).indexOf(chunk.offset + 1) >= 0;
        },
        // headers: {
        //   // 在header中添加的验证，请根据实际业务来
        //   "Access-Token": storage.get(ACCESS_TOKEN),
        // },
      },
      attrs: {
        accept: "image/*",
      },
      statusText: {
        success: "上传成功",
        error: "上传出错了",
        uploading: "上传中...",
        paused: "暂停中...",
        waiting: "等待中...",
        cmd5: "计算文件MD5中...",
      },
      fileList: [],
      disabled: true,
    };
  },
  watch: {
    fileList(o, n) {
      this.disabled = false;
    },
  },
  methods: {
    fileSuccess(rootFile, file, response, chunk) {
      // console.log(rootFile);
      // console.log(file);
      // console.log(message);
      // console.log(chunk);

      const result = JSON.parse(response);
      console.log(result);

      if (result.code == '200' && !this.skip) {
        axios
            .post(
                "//127.0.0.1:8160/upload/merge",
                {
                  identifier: file.uniqueIdentifier,
                  filename: file.name,
                  totalChunks: chunk.offset,
                },
                // {
                //   headers: { "Access-Token": storage.get(ACCESS_TOKEN) }
                // }
            )
            .then((res) => {
              if (res.data.success) {
                console.log("上传成功");
              } else {
                console.log(res);
              }
            })
            .catch(function (error) {
              console.log(error);
            });
      } else {
        console.log("上传成功，不需要合并");
      }
      if (this.skip) {
        this.skip = false;
      }
    },
    // fileSuccess(rootFile, file, response, chunk) {
    //   // console.log(rootFile);
    //   // console.log(file);
    //   // console.log(message);
    //   // console.log(chunk);
    //   const result = JSON.parse(response);
    //   console.log(result.success, this.skip);
    //   const user = {
    //     identifier: file.uniqueIdentifier,
    //     filename: file.name,
    //     totalChunks: chunk.offset,
    //   }
    //   if (result.success && !this.skip) {
    //     upload(user).then((res) => {
    //       if (res.code == 200) {
    //         console.log("上传成功");
    //       } else {
    //         console.log(res);
    //       }
    //     })
    //       .catch(function (error) {
    //         console.log(error);
    //       });
    //   } else {
    //     console.log("上传成功，不需要合并");
    //   }
    //   if (this.skip) {
    //     this.skip = false;
    //   }
    // },
    fileComplete(rootFile) {
      // 一个根文件（文件夹）成功上传完成。
      // console.log("fileComplete", rootFile);
      // console.log("一个根文件（文件夹）成功上传完成。");
    },
    complete() {
      // 上传完毕。
      // console.log("complete");
    },
    filesAdded(file, fileList, event) {
      // console.log(file);
      file.forEach((e) => {
        this.fileList.push(e);
        this.computeMD5(e);
      });
    },
    computeMD5(file) {
      let fileReader = new FileReader();
      let time = new Date().getTime();
      let blobSlice =
        File.prototype.slice ||
        File.prototype.mozSlice ||
        File.prototype.webkitSlice;
      let currentChunk = 0;
      const chunkSize = 1024 * 1024;
      let chunks = Math.ceil(file.size / chunkSize);
      let spark = new SparkMD5.ArrayBuffer();
      // 文件状态设为"计算MD5"
      file.cmd5 = true; //文件状态为“计算md5...”
      file.pause();
      loadNext();
      fileReader.onload = (e) => {
        spark.append(e.target.result);
        if (currentChunk < chunks) {
          currentChunk++;
          loadNext();
          // 实时展示MD5的计算进度
          console.log(
            `第${currentChunk}分片解析完成, 开始第${
              currentChunk + 1
            } / ${chunks}分片解析`
          );
        } else {
          let md5 = spark.end();
          console.log(
            `MD5计算完毕：${file.name} \nMD5：${md5} \n分片：${chunks} 大小:${
              file.size
            } 用时：${new Date().getTime() - time} ms`
          );
          spark.destroy(); //释放缓存
          file.uniqueIdentifier = md5; //将文件md5赋值给文件唯一标识
          file.cmd5 = false; //取消计算md5状态
          file.resume(); //开始上传
        }
      };
      fileReader.onerror = function () {
        this.error(`文件${file.name}读取出错，请检查该文件`);
        file.cancel();
      };
      function loadNext() {
        let start = currentChunk * chunkSize;
        let end =
          start + chunkSize >= file.size ? file.size : start + chunkSize;
        fileReader.readAsArrayBuffer(blobSlice.call(file.file, start, end));
      }
    },
    allStart() {
      console.log(this.fileList);
      this.fileList.map((e) => {
        if (e.paused) {
          e.resume();
        }
      });
    },
    allStop() {
      console.log(this.fileList);
      this.fileList.map((e) => {
        if (!e.paused) {
          e.pause();
        }
      });
    },
    allRemove() {
      this.fileList.map((e) => {
        e.cancel();
      });
      this.fileList = [];
    },
  },
};
</script>

<style>
.uploader-example {
  width: 100%;
  padding: 15px;
  margin: 0px auto 0;
  font-size: 12px;
  box-shadow: 0 0 10px rgba(0, 0, 0, 0.4);
}
.uploader-example .uploader-btn {
  margin-right: 4px;
}
.uploader-example .uploader-list {
  max-height: 440px;
  overflow: auto;
  overflow-x: hidden;
  overflow-y: auto;
}
</style>
```


## 3：后端实现
### 1）controller
```java
package com.xiaoze.exer.controller.FileUploadController;

import com.xiaoze.exer.entity.FileUpload.FileChunkDTO;
import com.xiaoze.exer.entity.FileUpload.FileChunkResultDTO;
import com.xiaoze.exer.service.IUploadService;
import com.xiaoze.exer.utils.Result;
import org.springframework.web.bind.annotation.*;

import javax.annotation.Resource;
 
/**
 * @ProjectName UploaderController
 * @author hhz
 * @version 1.0.0
 * @Description 附件分片上传
 * @createTime 2022/4/13 0013 15:58
 */
@RestController
@CrossOrigin
@RequestMapping("upload")
public class UploaderController {
 
   @Resource
   private IUploadService uploadService;
 
   /**
    * 检查分片是否存在
    *
    * @return
    */
   @GetMapping("chunk")
   public Result checkChunkExist(FileChunkDTO chunkDTO) {
      FileChunkResultDTO fileChunkCheckDTO;
      try {
         fileChunkCheckDTO = uploadService.checkChunkExist(chunkDTO);
         return Result.ok(fileChunkCheckDTO);
      } catch (Exception e) {
         return Result.fail(e.getMessage());
      }
   }
 
 
   /**
    * 上传文件分片
    *
    * @param chunkDTO
    * @return
    */
   @PostMapping("chunk")
   public Result uploadChunk(FileChunkDTO chunkDTO) {
      try {
         uploadService.uploadChunk(chunkDTO);
         return Result.ok(chunkDTO.getIdentifier());
      } catch (Exception e) {
         return Result.fail(e.getMessage());
      }
   }
 
   /**
    * 请求合并文件分片
    *
    * @param chunkDTO
    * @return
    */
   @PostMapping("merge")
   public Result mergeChunks(@RequestBody FileChunkDTO chunkDTO) {
      try {
         boolean success = uploadService.mergeChunk(chunkDTO.getIdentifier(), chunkDTO.getFilename(), chunkDTO.getTotalChunks());
         return Result.ok(success);
      } catch (Exception e) {
         return Result.fail(e.getMessage());
      }
   }
 
}
```

### 2）service
```java
package com.xiaoze.exer.service;


import com.xiaoze.exer.entity.FileUpload.FileChunkDTO;
import com.xiaoze.exer.entity.FileUpload.FileChunkResultDTO;

import java.io.IOException;
 
/**
 * @ProjectName IUploadService
 * @author hhz
 * @version 1.0.0
 * @Description 附件分片上传
 * @createTime 2022/4/13 0013 15:59
 */
public interface IUploadService {
 
   /**
    * 检查文件是否存在，如果存在则跳过该文件的上传，如果不存在，返回需要上传的分片集合
    * @param chunkDTO
    * @return
    */
   FileChunkResultDTO checkChunkExist(FileChunkDTO chunkDTO);
 
 
   /**
    * 上传文件分片
    * @param chunkDTO
    */
   void uploadChunk(FileChunkDTO chunkDTO) throws IOException;
 
 
   /**
    * 合并文件分片
    * @param identifier
    * @param fileName
    * @param totalChunks
    * @return
    * @throws IOException
    */
   boolean mergeChunk(String identifier,String fileName,Integer totalChunks)throws IOException;
}
```

### 3）impl
```java
package com.xiaoze.exer.service.impl;

import com.xiaoze.exer.entity.FileUpload.FileChunkDTO;
import com.xiaoze.exer.entity.FileUpload.FileChunkResultDTO;
import com.xiaoze.exer.service.IUploadService;
import org.apache.tomcat.util.http.fileupload.IOUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.data.redis.core.RedisTemplate;
import org.springframework.stereotype.Service;

import java.io.*;
import java.util.*;
 
 
/**
 * @ProjectName UploadServiceImpl
 * @author hhz
 * @version 1.0.0
 * @Description 附件分片上传
 * @createTime 2022/4/13 0013 15:59
 */
@Service
@SuppressWarnings("all")
public class UploadServiceImpl implements IUploadService {
 
   private Logger logger = LoggerFactory.getLogger(UploadServiceImpl.class);
 
   @Autowired
   private RedisTemplate<String, Object> redisTemplate;
 
   @Value("${uploadFolder}")
   private String uploadFolder;

   private static final String FILE_CHUNK_PREFIX = "FILE_CHUNK_";
 
   /**
    * 检查文件是否存在，如果存在则跳过该文件的上传，如果不存在，返回需要上传的分片集合
    *  检查分片是否存在
          ○ 检查目录下的文件是否存在。
          ○ 检查redis存储的分片是否存在。
          ○ 判断分片数量和总分片数量是否一致。
               如果文件存在并且分片上传完毕，标识已经完成附件的上传，可以进行秒传操作。
               如果文件不存在或者分片为上传完毕，则返回false并返回已经上传的分片信息。
    * @param chunkDTO
    * @return
    */
   @Override
   public FileChunkResultDTO checkChunkExist(FileChunkDTO chunkDTO) {
      //1.检查文件是否已上传过
      //1.1)检查在磁盘中是否存在
      // 分片地址
      String fileFolderPath = getFileFolderPath(chunkDTO.getIdentifier());
      logger.error("fileFolderPath-->{}", fileFolderPath);
      // 最终合成文件地址
      String filePath = getFilePath(chunkDTO.getIdentifier(), chunkDTO.getFilename());
      logger.error("filePath-->{}", filePath);
      File file = new File(filePath);
      boolean exists = file.exists();
      logger.error("exists-->{}", exists);
      //1.2)检查Redis中是否存在,并且所有分片已经上传完成。
      Set<Integer> uploaded = (Set<Integer>) redisTemplate.opsForHash().get(FILE_CHUNK_PREFIX+":"+chunkDTO.getIdentifier(), "uploaded");
      if (uploaded != null && uploaded.size() == chunkDTO.getTotalChunks() && exists) {
         return new FileChunkResultDTO(true);
      }
      File fileFolder = new File(fileFolderPath);
      if (!fileFolder.exists()) {
         boolean mkdirs = fileFolder.mkdirs();
         logger.info("准备工作,创建文件夹,fileFolderPath:{},mkdirs:{}", fileFolderPath, mkdirs);
      }
      // 断点续传，返回已上传的分片
      return new FileChunkResultDTO(false, uploaded);
   }
 
 
   /**
    * 上传分片
    *  上传附件分片
           ○ 判断目录是否存在，如果不存在则创建目录。
           ○ 进行切片的拷贝，将切片拷贝到指定的目录。
           ○ 将该分片写入redis
    * @param chunkDTO
    */
   @Override
   public void uploadChunk(FileChunkDTO chunkDTO) {
      //分块的目录
      String chunkFileFolderPath = getChunkFileFolderPath(chunkDTO.getIdentifier());
      logger.info("分块的目录 -> {}", chunkFileFolderPath);
      File chunkFileFolder = new File(chunkFileFolderPath);
      if (!chunkFileFolder.exists()) {
         boolean mkdirs = chunkFileFolder.mkdirs();
         logger.info("创建分片文件夹:{}", mkdirs);
      }
      //写入分片
      try (
              InputStream inputStream = chunkDTO.getFile().getInputStream();
              FileOutputStream outputStream = new FileOutputStream(new File(chunkFileFolderPath + chunkDTO.getChunkNumber()))
      ) {
         IOUtils.copy(inputStream, outputStream);
         logger.info("文件标识:{},chunkNumber:{}", chunkDTO.getIdentifier(), chunkDTO.getChunkNumber());
         //将该分片写入redis
         long size = saveToRedis(chunkDTO);
      } catch (Exception e) {
         e.printStackTrace();
      }
   }
 
 
   @Override
   public boolean mergeChunk(String identifier, String fileName, Integer totalChunks) throws IOException {
      return mergeChunks(identifier, fileName, totalChunks);
   }
 
   /**
    * 合并分片
    *
    * @param identifier
    * @param filename
    */
   private boolean mergeChunks(String identifier, String filename, Integer totalChunks) {
      // 分块的目录
      String chunkFileFolderPath = getChunkFileFolderPath(identifier);
      String filePath = getFilePath(identifier, filename);
      // 检查分片是否都存在
      if (checkChunks(chunkFileFolderPath, totalChunks)) {
         File chunkFileFolder = new File(chunkFileFolderPath);
         File mergeFile = new File(filePath);
         File[] chunks = chunkFileFolder.listFiles();
         // 切片排序1、2/3、---
         List fileList = Arrays.asList(chunks);
         Collections.sort(fileList, (Comparator<File>) (o1, o2) -> {
            return Integer.parseInt(o1.getName()) - (Integer.parseInt(o2.getName()));
         });
         try {
            RandomAccessFile randomAccessFileWriter = new RandomAccessFile(mergeFile, "rw");
            byte[] bytes = new byte[1024];
            for (File chunk : chunks) {
               RandomAccessFile randomAccessFileReader = new RandomAccessFile(chunk, "r");
               int len;
               while ((len = randomAccessFileReader.read(bytes)) != -1) {
                  randomAccessFileWriter.write(bytes, 0, len);
               }
               randomAccessFileReader.close();
            }
            randomAccessFileWriter.close();
         } catch (Exception e) {
            return false;
         }
         return true;
      }
      return false;
   }
 
   /**
    * 检查分片是否都存在
    * @param chunkFileFolderPath
    * @param totalChunks
    * @return
    */
   private boolean checkChunks(String chunkFileFolderPath, Integer totalChunks) {
      try {
         for (int i = 1; i <= totalChunks + 1; i++) {
            File file = new File(chunkFileFolderPath + File.separator + i);
            if (file.exists()) {
               continue;
            } else {
               return false;
            }
         }
      } catch (Exception e) {
         return false;
      }
      return true;
   }
 
   /**
    * 分片写入Redis
    * 判断切片是否已存在，如果未存在，则创建基础信息，并保存。
    * @param chunkDTO
    */
   private synchronized long saveToRedis(FileChunkDTO chunkDTO) {
      Set<Integer> uploaded = (Set<Integer>) redisTemplate.opsForHash().get(FILE_CHUNK_PREFIX+":"+chunkDTO.getIdentifier(), "uploaded");
      if (uploaded == null) {
         uploaded = new HashSet<>(Arrays.asList(chunkDTO.getChunkNumber()));
         HashMap<String, Object> objectObjectHashMap = new HashMap<>();
         objectObjectHashMap.put("uploaded", uploaded);
         objectObjectHashMap.put("totalChunks", chunkDTO.getTotalChunks());
         objectObjectHashMap.put("totalSize", chunkDTO.getTotalSize());
//            objectObjectHashMap.put("path", getFileRelativelyPath(chunkDTO.getIdentifier(), chunkDTO.getFilename()));
         objectObjectHashMap.put("path", chunkDTO.getFilename());
         redisTemplate.opsForHash().putAll(FILE_CHUNK_PREFIX+":"+chunkDTO.getIdentifier(), objectObjectHashMap);
      } else {
         uploaded.add(chunkDTO.getChunkNumber());
         redisTemplate.opsForHash().put(FILE_CHUNK_PREFIX+":"+chunkDTO.getIdentifier(), "uploaded", uploaded);
      }
      return uploaded.size();
   }
 
   /**
    * 得到文件的绝对路径
    *
    * @param identifier
    * @param filename
    * @return
    */
   private String getFilePath(String identifier, String filename) {
      String ext = filename.substring(filename.lastIndexOf("."));
       //return getFileFolderPath(identifier) + identifier + ext;  //G:\\FileUploadTest\\test\\a\\b\\ab14lskhjkhkjkajsdba\\
      return uploadFolder + filename;  //
   }
 
   /**
    * 得到文件的相对路径
    *
    * @param identifier
    * @param filename
    * @return
    */
   private String getFileRelativelyPath(String identifier, String filename) {
      String ext = filename.substring(filename.lastIndexOf("."));
      return "/" + identifier.substring(0, 1) + "/" +
              identifier.substring(1, 2) + "/" +
              identifier + "/" + identifier
              + ext;
   }
 
 
   /**
    * 得到分块文件所属的目录
    *
    * @param identifier
    * @return
    */
   private String getChunkFileFolderPath(String identifier) {
      return getFileFolderPath(identifier) + "chunks" + File.separator;
   }
 
   /**
    * 得到文件所属的目录
    *
    * @param identifier
    * @return
    */
   private String getFileFolderPath(String identifier) {

      // uploadFolder = G:\\FileUploadTest\\test
      // identifier = ab14lskhjkhkjkajsdba
      return uploadFolder  + identifier.substring(0, 1) + File.separator +
              identifier.substring(1, 2) + File.separator +
              identifier + File.separator;  //  G:\\FileUploadTest\\test\\a\\b\\ab14lskhjkhkjkajsdba\\
//        return uploadFolder;
   }
}
```